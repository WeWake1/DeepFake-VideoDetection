# Project Structure Guide - DeepFake Detection

**Date:** November 11, 2025  
**Purpose:** Understand what's in each folder and which files are critical

---

## ðŸ“ **Complete Directory Structure**

```
J:\DF/
â”œâ”€â”€ ðŸ“¦ checkpoints/              # Trained model weights
â”œâ”€â”€ ðŸ“¦ co/                       # Preprocessing scripts
â”œâ”€â”€ ðŸ“¦ config/                   # Training configuration
â”œâ”€â”€ ðŸ“¦ data/                     # Dataset mappings & metadata
â”œâ”€â”€ ðŸ“¦ docs/                     # Documentation
â”œâ”€â”€ ðŸ“¦ logs/                     # Training logs
â”œâ”€â”€ ðŸ“¦ paper/                    # Research paper materials
â”œâ”€â”€ ðŸ“¦ results/                  # Model predictions & analysis
â”œâ”€â”€ ðŸ“¦ scripts/                  # Analysis & utility scripts
â”œâ”€â”€ ðŸ“¦ train/                    # Core training pipeline
â”œâ”€â”€ ðŸ“¦ .venv/                    # Python environment (ignore)
â”œâ”€â”€ ðŸ“¦ DS/                       # Source dataset videos
â”œâ”€â”€ ðŸ“¦ FR/                       # Extracted frames (on J: drive)
â”œâ”€â”€ ðŸ“¦ Celeb-synthesis FAKE FRAMES-1/  # Frame backup
â”œâ”€â”€ ðŸ“¦ friend's architecture/    # Friend's code (reference only)
â”œâ”€â”€ ðŸ“„ README.md                 # Main documentation
â”œâ”€â”€ ðŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ðŸ“„ .gitignore               # Git ignore rules
â””â”€â”€ ðŸ“„ QUICK_REFERENCE.md       # Command cheatsheet
```

---

## ðŸ”¥ **MOST IMPORTANT FILES** (Top Priority)

### **1. Core Training Pipeline** â­â­â­â­â­

| File | Purpose | Lines | Importance |
|------|---------|-------|------------|
| **`train/train.py`** | Main training script - orchestrates entire training process | 310 | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`train/models.py`** | Model architecture - defines your dual-stream EfficientNet + ConvLSTM | 248 | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`train/dataset.py`** | Data loading - loads video pairs and creates training batches | 245 | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ VERY IMPORTANT |
| **`train/inference.py`** | Testing & evaluation - runs model on test videos | 358 | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ VERY IMPORTANT |
| **`train/utils.py`** | Helper functions - metrics, checkpointing, transforms | 198 | ðŸ”¥ðŸ”¥ðŸ”¥ IMPORTANT |

**What they do:**
- `train.py` â†’ Runs training loop, saves checkpoints, logs metrics
- `models.py` â†’ **YOUR ARCHITECTURE** (dual-stream EfficientNet-B4/B0 + ConvLSTM)
- `dataset.py` â†’ Loads face images from F: drive, creates 10-frame sequences
- `inference.py` â†’ Evaluates model on test set, generates predictions.csv
- `utils.py` â†’ Calculate accuracy/AUC, save checkpoints, apply transforms

---

### **2. Trained Model** â­â­â­â­â­

| File | Purpose | Size | Importance |
|------|---------|------|------------|
| **`checkpoints/best_model.pth`** | Your trained model (epoch 12, 100% val accuracy) | 230MB | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`checkpoints/last_model.pth`** | Final training checkpoint (epoch 22) | 230MB | ðŸ”¥ðŸ”¥ BACKUP |

**What they contain:**
- Model weights (58.5M parameters)
- Optimizer state
- Training epoch number
- Validation metrics

**Best model stats:**
- Epoch: 12
- Val Loss: 0.0001
- Val Accuracy: 100%
- Test Accuracy: 100% (1,646/1,646)

---

### **3. Configuration** â­â­â­â­

| File | Purpose | Size | Importance |
|------|---------|------|------------|
| **`config/defaults.yaml`** | All hyperparameters and paths | ~100 lines | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |

**What it contains:**
```yaml
data:
  real_path: F:/real          # Face images location
  fake_path: F:/fake
  sequence_length: 10         # 10-frame sequences
  frame_skip: 3              # Every 3rd frame

model:
  hq_backbone: efficientnet-b4  # High-quality stream
  lq_backbone: efficientnet-b0  # Low-quality stream
  lstm_hidden: 256           # ConvLSTM layer 1
  lstm_hidden_2: 128         # ConvLSTM layer 2

training:
  batch_size: 16
  num_workers: 8
  lr: 0.0001
  epochs: 100
  early_stop_patience: 10

preprocessing:
  hq_size: 224               # HQ stream resolution
  lq_size: 112               # LQ stream resolution
```

---

### **4. Dataset Mappings** â­â­â­â­

| File | Purpose | Rows | Importance |
|------|---------|------|------------|
| **`data/training_pairs.csv`** | 5,490 real-fake video pairs for training | 5,490 | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`data/enhanced_mapping.csv`** | Complete video metadata (frames, status, drive) | 11,229 | ðŸ”¥ðŸ”¥ðŸ”¥ IMPORTANT |
| **`data/frame_mapping.csv`** | Frame extraction results | 6,229 | ðŸ”¥ðŸ”¥ USEFUL |
| **`data/celebrity_mapping.json`** | Celebrity ID to video relationships | 59 celebs | ðŸ”¥ðŸ”¥ REFERENCE |

**training_pairs.csv structure:**
```csv
pair_id,real_video,fake_video,face_source_id,real_frames_path,fake_frames_path,...
1,id16_0000,id0_id16_0000,0,H:\Celeb-real FRAMES\id16_0000,J:\DF\FR\...
```

**This is what your model trains on!**

---

### **5. Training Results** â­â­â­â­

| File | Purpose | Size | Importance |
|------|---------|------|------------|
| **`logs/training_log.csv`** | Epoch-by-epoch training metrics | 22 rows | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`results/predictions.csv`** | Test set predictions (1,646 videos) | 1,646 rows | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ CRITICAL |
| **`results/metrics.json`** | Final test metrics (accuracy, AUC, etc.) | Small | ðŸ”¥ðŸ”¥ðŸ”¥ IMPORTANT |

**training_log.csv contains:**
```csv
epoch,train_loss,train_acc,train_auc,val_loss,val_acc,val_auc,lr,time
1,0.5234,0.7845,0.8567,0.4891,0.8123,0.8734,0.0001,245.3
...
12,0.0823,0.9956,0.9998,0.0001,1.0000,1.0000,0.0001,198.7  â† BEST
...
22,0.0456,0.9989,1.0000,0.0012,0.9994,1.0000,0.0001,187.2  â† STOPPED
```

---

## ðŸ“‚ **Folder-by-Folder Breakdown**

### **`checkpoints/`** - Trained Models

```
checkpoints/
â”œâ”€â”€ best_model.pth       # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Your winning model (epoch 12)
â””â”€â”€ last_model.pth       # ðŸ”¥ðŸ”¥ Final checkpoint (epoch 22)
```

**Purpose:** Stores trained model weights  
**Critical Files:** `best_model.pth` (230MB) - YOUR ENTIRE TRAINED MODEL  
**Used By:** `train/inference.py`, `scripts/test_individual_videos.py`

---

### **`co/`** - Preprocessing Scripts

```
co/
â”œâ”€â”€ face_detect_mtcnn_gpu(final).py    # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Face detection (GPU)
â”œâ”€â”€ framer_cpu(final)                  # ðŸ”¥ðŸ”¥ðŸ”¥ Frame extraction (CPU)
â”œâ”€â”€ create_mappings.py                 # ðŸ”¥ðŸ”¥ Generate training_pairs.csv
â”œâ”€â”€ verify_face_extraction.py          # ðŸ”¥ Check face extraction completeness
â””â”€â”€ FACE_DETECTION_README.md           # ðŸ”¥ Documentation
```

**Purpose:** Video â†’ Frames â†’ Faces preprocessing  
**Critical Files:**
- `face_detect_mtcnn_gpu(final).py` (407 lines) - Detects faces from frames, saves to F: drive
- `framer_cpu(final)` - Extracts frames from videos

**When to use:** Only when preprocessing NEW videos (not needed for training)

---

### **`config/`** - Configuration

```
config/
â””â”€â”€ defaults.yaml         # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ All hyperparameters
```

**Purpose:** Central configuration for training  
**Critical Files:** `defaults.yaml` - EVERYTHING is configured here  
**Used By:** All training/inference scripts

---

### **`data/`** - Dataset Mappings

```
data/
â”œâ”€â”€ training_pairs.csv              # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ 5,490 video pairs (CRITICAL)
â”œâ”€â”€ enhanced_mapping.csv            # ðŸ”¥ðŸ”¥ðŸ”¥ Complete video metadata
â”œâ”€â”€ frame_mapping.csv               # ðŸ”¥ðŸ”¥ Frame extraction results
â”œâ”€â”€ face_mapping.csv                # ðŸ”¥ Face detection results (empty?)
â”œâ”€â”€ celebrity_mapping.json          # ðŸ”¥ðŸ”¥ Celebrity ID relationships
â”œâ”€â”€ real_to_fake_mapping.json       # ðŸ”¥ Realâ†’Fake mappings
â”œâ”€â”€ face_detection_results.json     # ðŸ”¥ Face detection statistics
â””â”€â”€ face_extraction_verification.json  # ðŸ”¥ Completeness check
```

**Purpose:** Dataset metadata and relationships  
**Critical Files:**
- `training_pairs.csv` - **WHAT YOUR MODEL TRAINS ON** (5,490 pairs)
- `enhanced_mapping.csv` - Where each video's frames are located

---

### **`docs/`** - Documentation

```
docs/
â”œâ”€â”€ architecture.md              # ðŸ”¥ðŸ”¥ðŸ”¥ Model architecture explanation
â”œâ”€â”€ INSTALLATION.md              # ðŸ”¥ðŸ”¥ Setup guide
â”œâ”€â”€ TRAINING_MONITOR.md          # ðŸ”¥ Training progress guide
â”œâ”€â”€ RUN_VALIDATION.md            # ðŸ”¥ Validation commands
â”œâ”€â”€ GPU_USAGE_GUIDE.md           # ðŸ”¥ GPU optimization
â”œâ”€â”€ CLEANUP_COMMANDS.md          # File organization
â””â”€â”€ organization_report.txt      # Workspace structure report
```

**Purpose:** Documentation and guides  
**Critical Files:**
- `architecture.md` - Explains your dual-stream model design
- `INSTALLATION.md` - How to set up PyTorch CUDA

---

### **`logs/`** - Training Logs

```
logs/
â””â”€â”€ training_log.csv      # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Epoch-by-epoch metrics (22 epochs)
```

**Purpose:** Training history  
**Critical Files:** `training_log.csv` - Used for plotting training curves  
**What it tracks:** Loss, accuracy, AUC per epoch (train + val)

---

### **`paper/`** - Research Paper Materials

```
paper/
â”œâ”€â”€ methodology/
â”‚   â”œâ”€â”€ architecture.md       # ðŸ”¥ðŸ”¥ðŸ”¥ Model design explanation
â”‚   â”œâ”€â”€ INSTALLATION.md       # Setup instructions
â”‚   â””â”€â”€ README.md            # Overview
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_log.csv     # ðŸ”¥ðŸ”¥ðŸ”¥ Training metrics (copy)
â”‚   â””â”€â”€ best_model.pth       # Model checkpoint (copy)
â”œâ”€â”€ code_reference/
â”‚   â”œâ”€â”€ train.py             # Training script (copy)
â”‚   â”œâ”€â”€ models.py            # ðŸ”¥ðŸ”¥ðŸ”¥ Architecture (copy)
â”‚   â”œâ”€â”€ dataset.py           # Data loading (copy)
â”‚   â”œâ”€â”€ inference.py         # Evaluation (copy)
â”‚   â””â”€â”€ defaults.yaml        # Config (copy)
â”œâ”€â”€ data_description/
â”‚   â”œâ”€â”€ training_pairs.csv   # Dataset pairs (copy)
â”‚   â”œâ”€â”€ enhanced_mapping.csv # Metadata (copy)
â”‚   â””â”€â”€ frame_mapping.csv    # Frame info (copy)
â””â”€â”€ figures/
    â””â”€â”€ (empty - needs plots)
```

**Purpose:** Organized materials for writing research paper  
**Critical Files:**
- `methodology/architecture.md` - Draft of methodology section
- `code_reference/models.py` - Reference for explaining architecture
- `figures/` - **NEEDS WORK** (training curves, architecture diagram)

---

### **`results/`** - Model Predictions

```
results/
â”œâ”€â”€ predictions.csv          # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ 1,646 test predictions
â”œâ”€â”€ metrics.json            # ðŸ”¥ðŸ”¥ðŸ”¥ Test accuracy, AUC, confusion matrix
â””â”€â”€ interesting_videos.txt  # ðŸ”¥ðŸ”¥ Edge cases for inspection
```

**Purpose:** Test set evaluation results  
**Critical Files:**
- `predictions.csv` - Every test video's prediction + confidence
- `metrics.json` - Overall performance (100% accuracy!)

**predictions.csv structure:**
```csv
video_name,true_label,prediction,score
id49_0009,0,0,0.000003937  â† Real video, predicted Real (99.9996% confident)
id53_id49_0009,1,1,0.9999963  â† Fake video, predicted Fake (99.9996% confident)
```

---

### **`scripts/`** - Analysis Scripts

```
scripts/
â”œâ”€â”€ investigate_accuracy.py      # ðŸ”¥ðŸ”¥ðŸ”¥ Analyzes 100% accuracy (207 lines)
â””â”€â”€ test_individual_videos.py    # ðŸ”¥ðŸ”¥ðŸ”¥ Edge case analysis (299 lines)
```

**Purpose:** Post-training analysis and validation  
**Critical Files:**
- `investigate_accuracy.py` - Checks for overfitting, data leakage
- `test_individual_videos.py` - Finds hardest/easiest videos

**When to use:** After training, for paper analysis

---

### **`train/`** - Core Training Code

```
train/
â”œâ”€â”€ train.py          # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Main training script (310 lines)
â”œâ”€â”€ models.py         # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Model architecture (248 lines)
â”œâ”€â”€ dataset.py        # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Data loader (245 lines)
â”œâ”€â”€ inference.py      # ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ Evaluation (358 lines)
â”œâ”€â”€ utils.py          # ðŸ”¥ðŸ”¥ðŸ”¥ Helper functions (198 lines)
â”œâ”€â”€ __init__.py       # Package init
â”œâ”€â”€ README.md         # Training guide
â””â”€â”€ __pycache__/      # Python cache (ignore)
```

**Purpose:** YOUR ENTIRE TRAINING PIPELINE  
**Critical Files:** All 5 .py files are essential!

**Dependency flow:**
```
train.py
  â”œâ”€ uses models.py (create_model)
  â”œâ”€ uses dataset.py (DeepfakeDataset)
  â””â”€ uses utils.py (calculate_metrics, save_checkpoint)

inference.py
  â”œâ”€ uses models.py (create_model)
  â””â”€ uses utils.py (calculate_metrics, load_checkpoint)
```

---

## ðŸŽ¯ **THE 10 MOST CRITICAL FILES**

If you could only keep 10 files, these are THE ESSENTIAL ONES:

### **Rank 1-5: CANNOT FUNCTION WITHOUT THESE**

1. **`train/models.py`** - YOUR ARCHITECTURE (dual-stream EfficientNet + ConvLSTM)
2. **`checkpoints/best_model.pth`** - YOUR TRAINED MODEL (230MB)
3. **`config/defaults.yaml`** - ALL HYPERPARAMETERS
4. **`data/training_pairs.csv`** - WHAT YOU TRAINED ON (5,490 pairs)
5. **`train/train.py`** - HOW TO TRAIN THE MODEL

### **Rank 6-10: VERY IMPORTANT BUT RECOVERABLE**

6. **`train/inference.py`** - How to evaluate/test
7. **`train/dataset.py`** - How to load data
8. **`logs/training_log.csv`** - Training history (for plots)
9. **`results/predictions.csv`** - Test results (100% accuracy proof)
10. **`results/metrics.json`** - Overall metrics

---

## ðŸ“Š **File Size Analysis**

### **Large Files (>100MB):**
```
checkpoints/best_model.pth          230 MB  ðŸ”¥ Your trained model
checkpoints/last_model.pth          230 MB  Backup checkpoint
```

### **Medium Files (1-100MB):**
```
data/face_detection_results.json    ~50 MB  Face detection logs
data/celebrity_mapping.json         ~40 MB  Celebrity relationships
data/real_to_fake_mapping.json      ~35 MB  Mapping data
```

### **Small Files (<1MB):**
```
All .py scripts                     <1 KB each
All .csv files                      <10 MB each
All .md docs                        <100 KB each
```

---

## ðŸ”„ **File Usage Flow**

### **Training Pipeline:**
```
1. config/defaults.yaml
   â†“ (loads config)
2. train/train.py
   â†“ (imports)
3. train/models.py (creates model)
4. train/dataset.py (loads data from data/training_pairs.csv)
   â†“ (reads face images from F:/real/ and F:/fake/)
5. Training loop runs
   â†“ (saves checkpoints)
6. checkpoints/best_model.pth
   â†“ (logs metrics)
7. logs/training_log.csv
```

### **Inference Pipeline:**
```
1. train/inference.py
   â†“ (loads model)
2. checkpoints/best_model.pth
   â†“ (loads config)
3. config/defaults.yaml
   â†“ (loads test pairs)
4. data/training_pairs.csv (uses last 15% as test)
   â†“ (runs predictions)
5. results/predictions.csv
6. results/metrics.json
```

### **Preprocessing Pipeline (for NEW videos):**
```
1. Raw video.mp4
   â†“ (extract frames)
2. co/framer_cpu(final)
   â†“ (frames â†’ faces)
3. co/face_detect_mtcnn_gpu(final).py
   â†“ (saves to)
4. F:/real/video_name/*.jpg (aligned face crops)
   â†“ (can now be used by)
5. train/inference.py (for detection)
```

---

## ðŸŽ“ **For Your Paper**

### **Files You'll Reference:**

**Methodology Section:**
- `train/models.py` - Architecture details
- `config/defaults.yaml` - Hyperparameters
- `paper/methodology/architecture.md` - Design explanation

**Results Section:**
- `logs/training_log.csv` - Training curves
- `results/metrics.json` - Final performance
- `results/predictions.csv` - Per-video results

**Data Description:**
- `data/training_pairs.csv` - Dataset composition
- `data/enhanced_mapping.csv` - Video statistics

---

## ðŸ—‘ï¸ **Files You Can Ignore**

### **Not Important:**
- `.venv/` - Python environment (don't touch)
- `__pycache__/` - Python cache (auto-generated)
- `friend's architecture/` - Reference only
- `DS/` - Original videos (already processed)
- `FR/` - Frame backup (faces on F: drive)
- `Celeb-synthesis FAKE FRAMES-1/` - Frame backup

### **Temporary/Generated:**
- `organization_report.txt` - Just documentation
- `CLEANUP_COMMANDS.md` - Just instructions
- `organize_files.py` - Already ran

---

## ðŸš€ **Quick Action Guide**

### **Want to understand the architecture?**
â†’ Read: `train/models.py` (248 lines)

### **Want to retrain the model?**
â†’ Run: `train/train.py` (uses `config/defaults.yaml`)

### **Want to test on new videos?**
â†’ Run: `train/inference.py` (needs faces in F:/real/ or F:/fake/)

### **Want to analyze results?**
â†’ Read: `results/predictions.csv`, `results/metrics.json`

### **Want to write your paper?**
â†’ Use: `paper/` folder (methodology, results, figures)

### **Want to preprocess new videos?**
â†’ Run: `co/framer_cpu(final)` â†’ `co/face_detect_mtcnn_gpu(final).py`

---

## ðŸ“‹ **Summary Table**

| Folder | # Files | Most Important File | Purpose | Criticality |
|--------|---------|-------------------|---------|-------------|
| **train/** | 7 | `models.py` | Core training code | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **checkpoints/** | 2 | `best_model.pth` | Trained models | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **config/** | 1 | `defaults.yaml` | Configuration | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **data/** | 9 | `training_pairs.csv` | Dataset mappings | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **logs/** | 1 | `training_log.csv` | Training history | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **results/** | 3 | `predictions.csv` | Test results | ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ |
| **scripts/** | 2 | `investigate_accuracy.py` | Analysis tools | ðŸ”¥ðŸ”¥ðŸ”¥ |
| **co/** | 5 | `face_detect_mtcnn_gpu(final).py` | Preprocessing | ðŸ”¥ðŸ”¥ðŸ”¥ |
| **docs/** | 7 | `architecture.md` | Documentation | ðŸ”¥ðŸ”¥ |
| **paper/** | ~15 | `methodology/architecture.md` | Paper materials | ðŸ”¥ðŸ”¥ |

---

## âœ… **Key Takeaways**

1. **Your model is in:** `train/models.py` (248 lines of pure architecture)
2. **Your trained weights are in:** `checkpoints/best_model.pth` (230MB)
3. **Your training data is defined in:** `data/training_pairs.csv` (5,490 pairs)
4. **Your results are in:** `results/predictions.csv` (1,646 test samples, 100% accuracy)
5. **Everything is configured in:** `config/defaults.yaml` (hyperparameters, paths)

**To understand your entire project, read these 5 files in order:**
1. `config/defaults.yaml` (configuration)
2. `train/models.py` (architecture)
3. `train/dataset.py` (data loading)
4. `train/train.py` (training loop)
5. `train/inference.py` (evaluation)

**That's ~1,400 lines of code that define your entire system!** ðŸŽ‰

---

Does this help clarify the structure? Would you like me to dive deeper into any specific folder or file?
