"""
Generate architecture diagram for dual-stream deepfake detection model.

Creates a publication-quality visual representation showing:
- HQ stream (EfficientNet-B4, 224Ã—224)
- LQ stream (EfficientNet-B0, 112Ã—112)  
- ConvLSTM temporal modeling
- Attention fusion
- Classification head

Saves to paper/figures/architecture.png
"""

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Circle
import numpy as np
from pathlib import Path

# Set style
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'normal'

# Output
output_dir = Path(r'J:\DF\paper\figures')
output_dir.mkdir(parents=True, exist_ok=True)

print("=" * 70)
print("GENERATING ARCHITECTURE DIAGRAM")
print("=" * 70)
print()

# Create figure
fig, ax = plt.subplots(figsize=(16, 10))
ax.set_xlim(0, 16)
ax.set_ylim(0, 10)
ax.axis('off')

# Colors
color_input = '#E3F2FD'      # Light blue
color_hq = '#2196F3'         # Blue
color_lq = '#FF9800'         # Orange
color_temporal = '#9C27B0'   # Purple
color_fusion = '#4CAF50'     # Green
color_output = '#F44336'     # Red
color_text = '#212121'       # Dark gray

def draw_box(ax, x, y, width, height, color, label, sublabel='', alpha=0.8, edge_color='black', linewidth=2):
    """Draw a fancy box with label."""
    box = FancyBboxPatch(
        (x, y), width, height,
        boxstyle="round,pad=0.05",
        edgecolor=edge_color,
        facecolor=color,
        alpha=alpha,
        linewidth=linewidth
    )
    ax.add_patch(box)
    
    # Main label
    ax.text(x + width/2, y + height/2 + 0.15, label,
            ha='center', va='center', fontsize=11, fontweight='bold', color=color_text)
    
    # Sublabel
    if sublabel:
        ax.text(x + width/2, y + height/2 - 0.15, sublabel,
                ha='center', va='center', fontsize=9, color=color_text, style='italic')

def draw_arrow(ax, x1, y1, x2, y2, color='black', linewidth=2, style='->', label=''):
    """Draw arrow between boxes."""
    arrow = FancyArrowPatch(
        (x1, y1), (x2, y2),
        arrowstyle=style,
        color=color,
        linewidth=linewidth,
        mutation_scale=20,
        alpha=0.7
    )
    ax.add_patch(arrow)
    
    # Label on arrow
    if label:
        mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2
        ax.text(mid_x, mid_y + 0.2, label, ha='center', va='bottom',
                fontsize=8, color=color, fontweight='bold')

# ============================================================================
# TITLE
# ============================================================================
ax.text(8, 9.5, 'Dual-Stream Multi-Quality Deepfake Detection Architecture',
        ha='center', va='top', fontsize=16, fontweight='bold', color=color_text)
ax.text(8, 9.1, 'Combined Spatial-Temporal with Attention Fusion',
        ha='center', va='top', fontsize=12, color='gray', style='italic')

# ============================================================================
# INPUT LAYER
# ============================================================================
draw_box(ax, 6.5, 7.5, 3, 1, color_input, 'Video Input', 'T=10 frames (stride=3)')

# Split arrow
draw_arrow(ax, 8, 7.5, 5, 6.8, color=color_hq, linewidth=2)
draw_arrow(ax, 8, 7.5, 11, 6.8, color=color_lq, linewidth=2)

# ============================================================================
# HQ STREAM (LEFT)
# ============================================================================
# HQ Input
draw_box(ax, 2, 6, 2.5, 0.6, color_input, 'HQ Sequence', '224Ã—224Ã—3')

# HQ CNN
draw_box(ax, 1.75, 4.8, 3, 1, color_hq, 'EfficientNet-B4', '19M params, 7Ã—7Ã—1792')
draw_arrow(ax, 3.25, 6, 3.25, 5.8, color=color_hq)

# HQ ConvLSTM 1
draw_box(ax, 1.75, 3.6, 3, 0.7, color_temporal, 'ConvLSTM-1', '256 filters, 3Ã—3 kernel')
draw_arrow(ax, 3.25, 4.8, 3.25, 4.3, color=color_temporal)

# HQ ConvLSTM 2
draw_box(ax, 1.75, 2.5, 3, 0.7, color_temporal, 'ConvLSTM-2', '128 filters, 3Ã—3 kernel')
draw_arrow(ax, 3.25, 3.6, 3.25, 3.2, color=color_temporal)

# HQ Global Pooling
draw_box(ax, 2, 1.5, 2.5, 0.6, color_hq, 'Global Avg Pool', '512-D vector')
draw_arrow(ax, 3.25, 2.5, 3.25, 2.1, color=color_hq)

# ============================================================================
# LQ STREAM (RIGHT)
# ============================================================================
# LQ Input
draw_box(ax, 11.5, 6, 2.5, 0.6, color_input, 'LQ Sequence', '112Ã—112Ã—3 + JPEG')

# LQ CNN
draw_box(ax, 11.25, 4.8, 3, 1, color_lq, 'EfficientNet-B0', '5.3M params, 7Ã—7Ã—1280')
draw_arrow(ax, 12.75, 6, 12.75, 5.8, color=color_lq)

# LQ ConvLSTM 1
draw_box(ax, 11.25, 3.6, 3, 0.7, color_temporal, 'ConvLSTM-1', '256 filters, 3Ã—3 kernel')
draw_arrow(ax, 12.75, 4.8, 12.75, 4.3, color=color_temporal)

# LQ ConvLSTM 2
draw_box(ax, 11.25, 2.5, 3, 0.7, color_temporal, 'ConvLSTM-2', '128 filters, 3Ã—3 kernel')
draw_arrow(ax, 12.75, 3.6, 12.75, 3.2, color=color_temporal)

# LQ Global Pooling
draw_box(ax, 11.5, 1.5, 2.5, 0.6, color_lq, 'Global Avg Pool', '512-D vector')
draw_arrow(ax, 12.75, 2.5, 12.75, 2.1, color=color_lq)

# ============================================================================
# FUSION LAYER
# ============================================================================
# Arrows to fusion
draw_arrow(ax, 4.5, 1.8, 6.5, 1.2, color=color_hq, linewidth=2.5)
draw_arrow(ax, 11.5, 1.8, 9.5, 1.2, color=color_lq, linewidth=2.5)

# Fusion box
draw_box(ax, 6.5, 0.7, 3, 0.8, color_fusion, 'Attention Fusion', 'Î±_HQ Â· Ïˆ_HQ + Î±_LQ Â· Ïˆ_LQ')

# ============================================================================
# CLASSIFICATION HEAD
# ============================================================================
draw_arrow(ax, 8, 0.7, 8, 0.4, color=color_fusion, linewidth=2.5)

# Classifier layers (stacked)
draw_box(ax, 6.8, 0.05, 2.4, 0.25, '#BDBDBD', 'Dense(512â†’256) + ReLU')
draw_box(ax, 7.0, -0.25, 2.0, 0.25, '#9E9E9E', 'Dense(256â†’128) + ReLU')
draw_box(ax, 7.3, -0.55, 1.4, 0.25, color_output, 'Dense(128â†’1) + Sigmoid', alpha=0.9)

# Output
ax.text(8, -1.0, 'Output: p(fake | video) âˆˆ [0, 1]',
        ha='center', va='center', fontsize=11, fontweight='bold',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='yellow', alpha=0.3))

# ============================================================================
# ANNOTATIONS
# ============================================================================
# Parameter counts
ax.text(3.25, 0.8, '19M + 34M = 53M params',
        ha='center', va='top', fontsize=8, color=color_hq, fontweight='bold')
ax.text(12.75, 0.8, '5.3M + 34M = 39M params',
        ha='center', va='top', fontsize=8, color=color_lq, fontweight='bold')

# Stream labels
ax.text(3.25, 7.2, 'HIGH-QUALITY STREAM', ha='center', va='bottom',
        fontsize=12, fontweight='bold', color=color_hq,
        bbox=dict(boxstyle='round,pad=0.3', facecolor=color_hq, alpha=0.2))
ax.text(12.75, 7.2, 'LOW-QUALITY STREAM', ha='center', va='bottom',
        fontsize=12, fontweight='bold', color=color_lq,
        bbox=dict(boxstyle='round,pad=0.3', facecolor=color_lq, alpha=0.2))

# Key features callouts
ax.text(0.3, 5.5, 'âœ“ Spatial artifact detection\nâœ“ High-resolution features\nâœ“ Fine-grained details',
        ha='left', va='center', fontsize=8, color=color_hq,
        bbox=dict(boxstyle='round,pad=0.4', facecolor=color_hq, alpha=0.1))

ax.text(15.7, 5.5, 'âœ“ Compression robustness\nâœ“ Real-world noise\nâœ“ Efficient computation',
        ha='right', va='center', fontsize=8, color=color_lq,
        bbox=dict(boxstyle='round,pad=0.4', facecolor=color_lq, alpha=0.1))

ax.text(0.3, 3.0, 'âœ“ Temporal consistency\nâœ“ Blink dynamics\nâœ“ Motion patterns',
        ha='left', va='center', fontsize=8, color=color_temporal,
        bbox=dict(boxstyle='round,pad=0.4', facecolor=color_temporal, alpha=0.1))

ax.text(15.7, 3.0, 'âœ“ Temporal consistency\nâœ“ Blink dynamics\nâœ“ Motion patterns',
        ha='right', va='center', fontsize=8, color=color_temporal,
        bbox=dict(boxstyle='round,pad=0.4', facecolor=color_temporal, alpha=0.1))

ax.text(15.7, 1.2, 'âœ“ Adaptive weighting\nâœ“ Stream confidence\nâœ“ Learned fusion',
        ha='right', va='center', fontsize=8, color=color_fusion,
        bbox=dict(boxstyle='round,pad=0.4', facecolor=color_fusion, alpha=0.1))

# Legend
legend_y = -1.5
ax.text(1, legend_y, 'â–  Input', fontsize=9, color=color_input, fontweight='bold')
ax.text(3, legend_y, 'â–  Spatial CNN', fontsize=9, color=color_hq, fontweight='bold')
ax.text(5.5, legend_y, 'â–  Temporal LSTM', fontsize=9, color=color_temporal, fontweight='bold')
ax.text(8.5, legend_y, 'â–  Fusion', fontsize=9, color=color_fusion, fontweight='bold')
ax.text(11, legend_y, 'â–  Output', fontsize=9, color=color_output, fontweight='bold')

# Model stats
stats_text = (
    'Total Parameters: 58.5M\n'
    'Training Time: 3.5 hours\n'
    'Test Accuracy: 100%\n'
    'Inference: ~0.5s/video'
)
ax.text(15.7, -1.5, stats_text,
        ha='right', va='top', fontsize=9, color='black',
        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.3))

plt.tight_layout()
output_file = output_dir / 'architecture_diagram.png'
plt.savefig(output_file, bbox_inches='tight', facecolor='white')
print(f"âœ… Architecture diagram saved: {output_file}")
print()
print("=" * 70)
print("DIAGRAM FEATURES")
print("=" * 70)
print("âœ“ Dual-stream layout (HQ left, LQ right)")
print("âœ“ Color-coded components (inputâ†’CNNâ†’LSTMâ†’fusionâ†’output)")
print("âœ“ Parameter counts for each stream")
print("âœ“ Tensor dimensions at each stage")
print("âœ“ Key features annotated")
print("âœ“ Model statistics included")
print("âœ“ Publication-quality (300 DPI)")
print()
print("Ready for paper Figure 1! ðŸŽ¨")
print()

plt.close()
